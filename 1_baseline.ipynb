{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Représentation Bag-of-Words et Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les librairies nécessaires pour ce TD,\n",
    "# dont le second bloc est tiers, i.e. pas installé\n",
    "# avec Python par défaut.\n",
    "# Localement, on aura donc recours à pip (dans la console\n",
    "# système et non la console python) pour les installer au\n",
    "# besoin (instructions en commentaire ci-dessous).\n",
    "\n",
    "import re  # librairie standard (i.e. livrée avec Python)\n",
    "\n",
    "import matplotlib.pyplot as plt  # pip install matplotlib>=3.2\n",
    "import numpy as np   # pip install numpy>=1.15\n",
    "import pandas as pd  # pip install pandas>=1.0\n",
    "import scipy         # pip install scipy>=1.4\n",
    "import sklearn       # pip install scikit-learn>=0.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Présentation des données\n",
    "\n",
    "On va utiliser un jeu de données open-source qui n'a rien à voir avec la santé : il s'agit de critiques de films sur IMDB, en anglais, accompagnées d'une note. Ces dernières vont normalement de 1 à 10, mais les notes égales à 5 et à 6 ont été exclues, afin de définir une variable de polarité : avis positif (>= 7) ou négatif (<= 4).\n",
    "\n",
    "L'objet va donc être de manipuler un peu ces données pour en construire une représentation numérique adaptée à la mise en oeuvre de modèles d'apprentissage supervisé pour une tâche de classificaiton binaire : la prédiction de la polarité de la critique, une tâche classique d'analyse de sentiment.\n",
    "\n",
    "Nous disposons d'un total de 50 000 échantillons, que j'ai répartis en trois jeux :\n",
    "* train : 35 000 (70%) échantillons, utilisés pour entraîner des modèles de classification binaire\n",
    "* valid : 12 500 (15%) échantillons, utilisés pour évaluer les modèles et en chercher les meilleurs hyper-paramètres\n",
    "* test  : 12 500 (15%) échantillons, utilisés pour l'évaluation finale du ou des modèles retenus\n",
    "\n",
    "source des données : [Large Movie Review Dataset v1.0](http://ai.stanford.edu/%7Eamaas/data/sentiment/)<br/>\n",
    "le notebook de préparation des données est fourni pour votre curiosité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement des données\n",
    "\n",
    "Les données sont stockées au format tsv (_tab-separated values_) ; on va utiliser [Pandas](https://pandas.pydata.org/) pour les lire et les manipuler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affiche les premières lignes (les textes seront tronqués).\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution des notes.\n",
    "%matplotlib inline  # spécifique à Jupyter Notebook\n",
    "\n",
    "train['rate'].value_counts().sort_index().plot.bar(\n",
    "    title='Effectifs des échantillons par note.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part des échantillons d'entraînement positifs.\n",
    "print(train['polarity'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples de manipulation de string\n",
    "\n",
    "Prenons un unique texte et manipulons-le un peu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train.loc[2, 'text']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passage en minuscule.\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrait de la ponctuation (sauf tirets, apostrophes et barres obliques).\n",
    "print(re.sub(r\"[^\\w\\s\\-'/]\", \"\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrait de certains mots peu informatifs (stopwords).\n",
    "# On voit qu'établir une liste est vite pénible !\n",
    "print(' '.join(\n",
    "    word for word in text.lower().split(' ')\n",
    "    if word not in ('i', 'in', 'it', 'at', 'but', 'and', 'for', 'or', 'the', 'a', 'is', 'are', 'were')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction de tous les groupes de trois mots (3-grams) non séparés par de la ponctuation.\n",
    "\n",
    "# (a) on crée une copie normalisée du texte\n",
    "tbis = re.sub(r\"[^\\w\\s\\.\\-'/]\", \"\", text.lower())  # on retire la ponctuation indésirable.\n",
    "tbis = tbis.replace('?', '.').replace('!', '.')  # on remplace certains signes\n",
    "tbis = re.sub('  +', ' ', tbis)  # on retire les espaces redondants\n",
    "tbis = re.sub(r'\\.\\.+', '\\.', tbis)  # on retire les points redondants\n",
    "tbis = tbis.replace(' .', '.')  # on retire les espaces avant un point\n",
    "print(tbis)\n",
    "\n",
    "# (b) on découpe en phrases ; pour chaque phrase on produit les trigrammes.\n",
    "trigrams = []\n",
    "for sentence in tbis.split('.'):\n",
    "    tokens = sentence.strip(' ').split(' ')\n",
    "    for i in range(len(tokens) - 2):\n",
    "        trigrams.append(\n",
    "            (tokens[i], tokens[i + 1], tokens[i + 2])\n",
    "        )\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Systématisons un peu\n",
    "\n",
    "Ici, on va :\n",
    "* définir et appliquer des premières règles de nettoyage pour tous les textes\n",
    "* explorer le vocabulaire des textes nettoyés et la fréquence des termes qui le composent\n",
    "* choisir un vocabulaire restreint à partir de critères de fréquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisons un peu nos textes.\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Apply basic normalization to a given text.\"\"\"\n",
    "    tbis = re.sub(r\"[^\\w\\s\\.\\-'/]\", \"\", text.lower())  # on retire la ponctuation indésirable.\n",
    "    tbis = tbis.replace(' - ', ' ')  # on ne garde que les tirets intra-mot\n",
    "    tbis = tbis.replace('?', '.').replace('!', '.')  # on remplace certains signes\n",
    "    tbis = re.sub('  +', ' ', tbis)  # on retire les espaces redondants\n",
    "    tbis = re.sub(r'\\.\\.+', '\\.', tbis)  # on retire les points redondants\n",
    "    tbis = tbis.replace(' .', '.')  # on retire les espaces avant un point\n",
    "    return tbis.strip('. ') + '.'   # on oblige à commencer par une lettre et finir par un point\n",
    "\n",
    "\n",
    "train['clean'] = train['text'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorons notre vocabulaire.\n",
    "# On va répertorier tous les mots contenus dans les textes, ainsi que leur nombre d'occurence.\n",
    "\n",
    "# Définissons une fonction pour ce faire.\n",
    "def get_vocab_count(series):\n",
    "    \"\"\"List terms and their number of occurences for a pandas.Series of texts.\"\"\"\n",
    "    # On définit un dictionaire (vide) pour contenir les couples ('mot': compte)\n",
    "    vocab = {}\n",
    "    # On définit une fonction qui met à jour le vocabulaire pour un texte donné.\n",
    "    # (jargon technique: cette fonction est une closure, qui a un effet de bord)\n",
    "    def count_vocab(text):\n",
    "        nonlocal vocab\n",
    "        for token in text.replace('.', ' ').split(' '):\n",
    "            token = token.strip(' ')\n",
    "            if token:  # on ne prend pas les chaînes vides\n",
    "                count = vocab.get(token, 0)  # compte actuel, ou 0 si nouveau terme\n",
    "                vocab[token] = count + 1     # mise à jour du compte (ou création)\n",
    "    # On applique la fonction à notre pandas.Series.\n",
    "    # Note: c'est plus rapide que de faire une boucle sur les textes.\n",
    "    series.apply(count_vocab)  # l'effet de bord met à jour 'vocab'\n",
    "    # On retourne le vocabulaire qu'on a construit, comme une pandas.Series.\n",
    "    return pd.Series(vocab)\n",
    "\n",
    "\n",
    "# Appliquons à nos textes (nettoyés).\n",
    "vocab = get_vocab_count(train['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre de termes distincts : %s\" % len(vocab))\n",
    "print(\"Nombre de termes apparaissant plus de 1 fois : %s\" % (vocab > 1).sum())\n",
    "print(\"Nombre de termes apparaissant plus de 10 fois : %s\" % (vocab > 10).sum())\n",
    "print(\"Nombre de termes apparaissant plus de 100 fois : %s\" % (vocab > 100).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisons l'histogramme de la (log-)fréquence des mots :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "np.log(vocab).plot.hist(ax=axes[0], title='all words')\n",
    "np.log(vocab[vocab > 1]).plot.hist(ax=axes[1], title='words appearing 2+ times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous connaissez la [loi de Zipf](https://fr.wikipedia.org/wiki/Loi_de_Zipf) ?\n",
    "Ici, c'est pire !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vocab.sort_values(ascending=False).values[:1000])\n",
    "plt.title(\"Nombre d'occurrence des 1000 tokens les plus fréquents selon leur rang.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regardons les vingt mots les plus fréquents. C'est peu édifiant.\n",
    "vocab.sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentation bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'est échauffés en faisant des choses manuellement, mais utilisons plutôt un outil clefs-en-main, qui sera plus performant, pour aller plus loin.\n",
    "\n",
    "On va utiliser la librairie [Scikit-Learn](https://scikit-learn.org/), qui nous fournira également par la suite des modèles d'apprentissage supervisé (hors réseaux de neurones). C'est une librairie très complète, mais surtout très bien documentée, et il y a beaucoup à apprendre sur l'analyse de données et le _machine learning_ sur leur site, qui se veut à la fois précis et pédagogue.\n",
    "\n",
    "On aurait presque pu remplacer ce tutoriel par [le leur](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) ; d'ailleurs notre sujet est suggéré en exercice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu'est-ce qu'on fait quand on a un bel outil dont on veut se servir mais qu'on ne connaît pas ? On lit [la documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dans la console standard, on ferait help(CountVectorizer)\n",
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En résumé, on peut :\n",
    "* retirer les acccents, passer en lowercase (True par défaut), utiliser des stopwords...\n",
    "* limiter la taille du vocabulaire, ou forcer l'usage d'un vocabulaire pré-défini\n",
    "* utiliser des n-grams pour n dans un intervalle donné (par défaut, unigrammes seulement)\n",
    "* changer la façon dont les tokens sont découpés (par défaut, deux lettres ou plus séparées par tout autre symbole ou espace)\n",
    "* générer des sacs de mots binaires (par défaut, on compte les occurences)\n",
    "* fixer des limites à la part de documents du corpus dans laquelle un token apparaît pour ne pas être écarté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On instancie un object 'CountVectorizer', paramétré comme il nous sied.\n",
    "count_vect = CountVectorizer(\n",
    "    lowercase=True,           # on passe le texte en minuscule\n",
    "    stop_words='english',     # liste existante de stopwords anglais à filtrer\n",
    "    token_pattern=r\"\\b[\\w\\-']+\\b\",  # on ne coupe pas les unigrammes autour des tirets et apostrophes\n",
    "    ngram_range=(1, 2),       # on considère les unigrammes et les bigrammes\n",
    "    min_df=(100 / len(train)) # on ne conserve que les tokens apparaissant dans au moins cent documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On établit le vocabulaire, puis on produite la matrice documents-termes.\n",
    "# Ceci peut prendre un peu de temps, malgré le beau travail d'optimisation derrière.\n",
    "dtm = count_vect.fit_transform(train['clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La DTM est sparse ; on peut néanmoins accéder à ses contenus.<br/>\n",
    "On va s'intéresser au vocabulaire établi, au nombre total d'occurences des tokens, et au nombre de documents distincts dans lesquels ils figurent.\n",
    "\n",
    "A noter : par convention, pour tous les modèles implémentés dans Scikit-Learn, les attributs dont le nom finit par '\\_' sont ceux créés pendant l'appel à la méthode `fit` de l'objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère le dictionnaire {terme: indice dans le vocabulaire} et on le renverse.\n",
    "vocab = {\n",
    "    idx: token for token, idx in count_vect.vocabulary_.items()\n",
    "}\n",
    "\n",
    "print(\"Number of tokens in the vocabulary: %s\" % len(vocab))\n",
    "print(\"Out of which %s are bigrams.\" % sum(' ' in token for token in vocab.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construisons un DataFrame avec des informations sur le vocabulaire.\n",
    "voc_df = pd.DataFrame({'token': vocab})\n",
    "voc_df['bigram'] = voc_df['token'].str.contains(' ')\n",
    "voc_df['counts'] = np.array(dtm.sum(axis=0))[0]\n",
    "voc_df['n_docs'] = np.array((dtm > 0).sum(axis=0))[0]\n",
    "\n",
    "voc_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Au passage, la DTM est-elle vraiment sparse ?\n",
    "# On quantifie la part de valeurs non-nulles:\n",
    "(dtm > 0).sum() / (dtm.shape[0] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quels sont les vingts tokens les plus fréquents ?\n",
    "voc_df.sort_values('counts', ascending=False).iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quels sont les vingts tokens apparaissant dans le plus de documents ?\n",
    "voc_df.sort_values('n_docs', ascending=False).iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quels sont les bigrammes les plus fréquents ?\n",
    "voc_df[voc_df['bigram']].sort_values('counts', ascending=False).iloc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests du chi-deux\n",
    "\n",
    "On va commencer par un test statistique simple, pour déterminer pour chacun des tokens retenus s'il y a une différence significative de sa fréquence selon que la critique est positive ou non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On extrait les indices des échantillons positifs et négatifs.\n",
    "pos = train.loc[train['polarity'] == 1].index\n",
    "neg = train.loc[train['polarity'] == 0].index\n",
    "# Pour chaque groupe, pour chaque token, on collecte\n",
    "# le nombre de documents dans lesquels il apparaît.\n",
    "voc_df['pos_docs'] = np.array((dtm[pos] > 0).sum(axis=0))[0]\n",
    "voc_df['neg_docs'] = np.array((dtm[neg] > 0).sum(axis=0))[0]\n",
    "\n",
    "voc_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On effectue, pour chaque mot, le test du chi-deux.\n",
    "from scipy.stats import chisquare  # third-party package (`pip install scipy`)\n",
    "\n",
    "voc_df['pvalue'] = chisquare(voc_df[['pos_docs', 'neg_docs']].values, axis=1).pvalue.round(5)\n",
    "\n",
    "voc_df.tail()\n",
    "\n",
    "print(\"Share of p-values below 0.05: %s\" % (voc_df['pvalue'] < 0.05).mean().round(4))\n",
    "print(\"Share of p-values below 0.01: %s\" % (voc_df['pvalue'] < 0.01).mean().round(4))\n",
    "print(\"Share of p-values below 0.001: %s\" % (voc_df['pvalue'] < 0.001).mean().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_df[voc_df['pvalue'] > 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_df[(voc_df['pvalue'] < 0.001) & (voc_df['pos_docs'] > voc_df['neg_docs'])].sort_values('pos_docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_df[(voc_df['pvalue'] < 0.001) & (voc_df['pos_docs'] < voc_df['neg_docs'])].sort_values('neg_docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraîne une régression logistique pénalisée par LASSO sur nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# On instancie le modèle; les options servent à utiliser la pénalisation LASSO.\n",
    "lreg = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "# On entraîne le modèle sur nos données.\n",
    "lreg.fit(dtm, train['polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On évalue le modèle sur le données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raccourci pour l'accuracy du modèle.\n",
    "lreg.score(dtm, train['polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si on veut faire les choses manuellement :\n",
    "y_true = train['polarity']\n",
    "y_pred = lreg.predict(dtm)\n",
    "\n",
    "print(\"True Positives : %s\" % y_pred[y_true == 1].sum())\n",
    "print(\"True Negatives : %s\" % (1 - y_pred[y_true == 0]).sum())\n",
    "print(\"False Positives : %s\" % y_pred[y_true == 0].sum())\n",
    "print(\"False Negatives : %s\" % (1 - y_pred[y_true == 1]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mais scikit-learn fournit déjà les outils.\n",
    "\n",
    "print(sklearn.metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais évaluons maintenant sur le jeu de validation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge les données de validation et on en produit la DTM\n",
    "# (selon le même vocabulaire de tokens).\n",
    "valid = pd.read_csv('data/valid.tsv', sep='\\t')\n",
    "valid['clean'] = valid['text'].apply(normalize_text)\n",
    "valid_dtm = count_vect.transform(valid['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On évalue. C'est correct, mais moins bien!\n",
    "y_true = valid['polarity']\n",
    "y_pred = lreg.predict(valid_dtm)\n",
    "\n",
    "print(sklearn.metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons un premier modèle, imparfait, qui peut nous servir de _baseline_ : nos efforts par la suite porteront sur le fait d'aller au-delà de ces performances, qui servent de référence de départ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sujet ouvert : comment faire mieux ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Premier exemple : sous-séléction informée des variables\n",
    "\n",
    "Une idée parmi d'autres : se restreindre aux termes qui semblent associés à l'output au sens du test d'indépendance du chi-deux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On entraîne une nouvelle régression, en réduisant le vocabulaire\n",
    "# sur la base de nos tests du chi-deux.\n",
    "lreg = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "idx = voc_df[voc_df['pvalue'] < 0.001].index.values\n",
    "lreg.fit(dtm[:, idx], train['polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation sur les données d'entraînement :\")\n",
    "print(sklearn.metrics.classification_report(\n",
    "    y_true=train['polarity'],\n",
    "    y_pred=lreg.predict(dtm[:, idx])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation sur les données de validation :\")\n",
    "print(sklearn.metrics.classification_report(\n",
    "    y_true=valid['polarity'],\n",
    "    y_pred=lreg.predict(valid_dtm[:, idx])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question : Comment interpréter ce petit changement dans les résultats ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Des idées en pagaille :\n",
    "\n",
    "Modifier les features :\n",
    "* étendre le vocabulaire à des n-grams plus longs\n",
    "* modifier les coefficients de la DTM : valeurs binaires ou valeurs TF-IDF (avec `sklearn.feature_extraction.text.TfidfTransformer`)\n",
    "* modifier les critères de sélection parmi le vocabulaire complet\n",
    "\n",
    "Modifier les hyper-paramètres du modèle :\n",
    "* essayer une autre pénalité que LASSO (l1)\n",
    "* modifier le coefficient de régularization (paramètre `C`)\n",
    "\n",
    "Modifier la classe du modèle :\n",
    "* arbre de décision (`sklearn.tree.DecisionTreeClassifier`)\n",
    "* RandomForest (`sklearn.ensemble.RandomForestClassifier`)\n",
    "* SVM (`sklearn.svm.SVC` ou `sklearn.svm.LinearSVC`)\n",
    "\n",
    "On peut évidemment combiner ces différents points (jouer sur les features _et_ la classe du modèle _et_ ses hyper-paramètres). Comment faire cela de manière un tant soit peu ordonnée ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Un peu de méthodologie :\n",
    "\n",
    "Un peu de vocabulaire essentiel en _Machine Learning_, mais un peu au-delà de ce TP.<br/>\n",
    "Aujourd'hui on pourra s'autoriser à opérer de manière moins structurée, mais ce sont de bonnes ressources en pratique.<br/>\n",
    "Un exemple d'utilisation est proposé dans le notebook \"2_sandbox_template\", que je vous invite à dupliquer et modifier de fond en comble pour mettre en place vos propres chaînes de pré-traitement et d'entraînement.\n",
    "\n",
    "* Validation croisée ([tutoriel scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html))\n",
    "    * intuition : on entraîne plusieurs fois un même modèle sur des sous-échantillons aléatoires, pour avoir une meilleure estimation de sa capacité à généraliser sur de nouvelles données\n",
    "* Grid Search ([tutoriel scikit-learn](https://scikit-learn.org/stable/modules/grid_search.html))\n",
    "    * intuition : on répète une même procédure d'entraînement, avec des hyper-paramètres distincts, pour trouver la meilleure combinaison de ces paramètres pour une classe de modèle et un jeu de données fixés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Et le word embedding?\n",
    "\n",
    "On y vient, dans un notebook distinct!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
