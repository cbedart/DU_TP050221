{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage supervisé sur une Matrice Document-Terme - sujet ouvert\n",
    "\n",
    "Je propose ici un petit template reprennant les principaux éléments du notebook \"1_baseline\" afin de mettre en place vos propres modèles d'analyse de sentiment sur les données.\n",
    "\n",
    "Ne pas hésiter à faire des copies pour séparer des approches distinctes ; sinon on peut faire autant d'expériences qu'on le souhaite dans un unique notebook, mais il faut réussir à ne pas se perdre entre toutes les variables (données, modèles, scores...) que l'on instancie !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Je remets tous les imports potentiellement utiles,\n",
    "# mais on peut être plus parcimonieux et/ou mieux\n",
    "# organisé et mettre cette cellule à jour pour\n",
    "# correspondre à tout ce qu'on utilise effectivement\n",
    "# dans ce notebook.\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge les données d'entraînement et validation.<br/>\n",
    "On extrait les cibles dans des variables explicitement nommées `y_[subset]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "valid = pd.read_csv('data/valid.tsv', sep='\\t')\n",
    "\n",
    "y_train = train['polarity'] \n",
    "y_valid = valid['polarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut optionnellement faire un peu de pré-traitement sur les données.<br/>\n",
    "Je remets la fonction utilisée dans le notebook baseline, qui peut être modifiée ou ignorée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Apply basic normalization to a given text.\"\"\"\n",
    "    tbis = re.sub(r\"[^\\w\\s\\.\\-'/]\", \"\", text.lower())  # on retire la ponctuation indésirable.\n",
    "    tbis = tbis.replace(' - ', ' ')  # on ne garde que les tirets intra-mot\n",
    "    tbis = tbis.replace('?', '.').replace('!', '.')  # on remplace certains signes\n",
    "    tbis = re.sub('  +', ' ', tbis)  # on retire les espaces redondants\n",
    "    tbis = re.sub(r'\\.\\.+', '\\.', tbis)  # on retire les points redondants\n",
    "    tbis = tbis.replace(' .', '.')  # on retire les espaces avant un point\n",
    "    return tbis.strip('. ') + '.'   # on oblige à commencer par une lettre et finir par un point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si l'on uncomment les deux lignes suivantes,\n",
    "# on remplace les textes par une version nettoyée.\n",
    "\n",
    "#train['text'] = train['text'].apply(normalize_text)\n",
    "#valid['text'] = valid['text'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je remets le code initial pour vectoriser le corpus en une DTM.<br/>\n",
    "C'est ici qu'il faut modifier et/ou ajouter du post-traitement pour changer les _features_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation du vectoriseur (note: on peut préférer le TfIdfVectorizer).\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    lowercase=True,           # on passe le texte en minuscule\n",
    "    stop_words='english',     # liste existante de stopwords anglais à filtrer\n",
    "    token_pattern=r\"\\b[\\w\\-']+\\b\",  # on ne coupe pas les unigrammes autour des tirets et apostrophes\n",
    "    ngram_range=(1, 2),       # on considère les unigrammes et les bigrammes\n",
    "    min_df=(100 / len(train)) # on ne conserve que les tokens apparaissant dans au moins cent documents\n",
    ")\n",
    "\n",
    "# Apprentissage et transformation sur train; transformation sur valid.\n",
    "X_train = count_vect.fit_transform(train['text'])\n",
    "X_valid = count_vect.transform(valid['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Des idées pour sous-sélectionner les variables (tokens) retenues ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les features établies (note: on peut en préparer des versions distinctes à tester et mettre en concurrence), on passe à la modélisation. Ici je remets l'exemple de la régression logistique, à modifier ou remplacer / étendre avec d'autres modèles.\n",
    "\n",
    "A noter, l'[API](https://fr.wikipedia.org/wiki/Interface_de_programmation) de Scikit-Learn est la même pour toutes les classes de modèles, donc on peut réutiliser la majeur partie des instructions (`clf.fit`, `clf.predict`, etc.) quelle que soit la classe de `clf` !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Evaluation sur les données d'entraînement :\")\n",
    "print(sklearn.metrics.classification_report(\n",
    "    y_true=y_train, y_pred=clf.predict(X_train)\n",
    "))\n",
    "print(\"Evaluation sur les données de validation :\")\n",
    "print(sklearn.metrics.classification_report(\n",
    "    y_true=y_valid, y_pred=clf.predict(X_valid)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faisons un petit peu de _tuning_ sur les hyper-paramètres du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par un grid search (avec cross-validation) sur le paramètre de régularisation pour la régression LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit la stratégie de grid-search.\n",
    "# Ici on prend une régression logistique LASSO,\n",
    "# et on teste différents paramètres \"C\".\n",
    "grid_cv = GridSearchCV(\n",
    "    estimator=LogisticRegression(penalty='l1', solver='liblinear'),\n",
    "    param_grid={'C': [0.01, 0.05, 0.1, 0.25, 0.5, 1.0]},\n",
    "    scoring=('accuracy', 'precision', 'recall', 'roc_auc'),\n",
    "    refit=False,\n",
    "    n_jobs=-1,  # on parallélise sur les CPUs disponibles\n",
    "    cv=5        # 5-fold cross-validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On exécute le grid search sur nos données d'entraînement\n",
    "# (la validation se fait par validation croisée 5-fold).\n",
    "grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On extrait les résultats sous forme d'un DataFrame.\n",
    "results = pd.DataFrame(grid_cv.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisons les scores selon le paramètre retenu.\n",
    "cols = ['params'] + [\n",
    "    '%s_test_%s' % (stat, score)\n",
    "    for score in ('roc_auc', 'accuracy', 'precision', 'recall')\n",
    "    for stat in ('mean', 'std')\n",
    "]\n",
    "results[cols].sort_values('mean_test_roc_auc', ascending=False).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut tout aussi bien chercher d'un même coup si la pénalisation L2 fait l'affaire, tout en explorant les paramètres de régularisation !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On essaie deux modèles : pénalisation L1 ou L2.\n",
    "# Dans chaque cas, on essaie de tuner le taux de régularisation.\n",
    "param_grid = [\n",
    "    {\n",
    "        'penalty': ['l1'], 'solver': ['liblinear'],\n",
    "        'C': [0.01, 0.05, 0.1, 0.25, 0.5, 1.0]\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'], 'solver': ['lbfgs'],\n",
    "        'C': [0.01, 0.05, 0.1, 0.25, 0.5, 1.0]\n",
    "    }\n",
    "]\n",
    "\n",
    "# On instancie le classifieur grid-search.\n",
    "grid_cv = GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid=param_grid,\n",
    "    scoring=('accuracy', 'precision', 'recall', 'roc_auc'),\n",
    "    refit=False,\n",
    "    n_jobs=-1,  # on parallélise sur les CPUs disponibles\n",
    "    cv=5        # 5-fold cross-validation\n",
    ")\n",
    "\n",
    "# On fait tourner le grid-search.\n",
    "grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualisons les résultats.\n",
    "results = pd.DataFrame(grid_cv.cv_results_)\n",
    "cols = ['param_penalty', 'param_C'] + [\n",
    "    '%s_test_%s' % (stat, score)\n",
    "    for score in ('roc_auc', 'accuracy', 'precision', 'recall')\n",
    "    for stat in ('mean', 'std')\n",
    "]\n",
    "results[cols].sort_values('mean_test_roc_auc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, on va choisir un modèle à partir de ces résultats, l'entraîner sur tout le jeu de train, et l'évaluer sur le jeu de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l1', solver='liblinear', C=0.25)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Evaluation sur les données d'entraînement :\")\n",
    "print(sklearn.metrics.classification_report(\n",
    "    y_true=y_train, y_pred=clf.predict(X_train)\n",
    "))\n",
    "print(\"Evaluation sur les données de validation :\")\n",
    "print(sklearn.metrics.classification_report(\n",
    "    y_true=y_valid, y_pred=clf.predict(X_valid)\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
